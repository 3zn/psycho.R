@article{team2000r,
  title={R language definition},
  author={Team, R Core},
  journal={Vienna, Austria: R foundation for statistical computing},
  year={2000}
}

@article{Etz2016,
abstract = {We revisit the results of the recent Reproducibility Project: Psychology by the Open Science Collaboration. We compute Bayes factors—a quantity that can be used to express comparative evidence for an hypothesis but also for the null hypothesis—for a large subset (N = 72) of the original papers and their corresponding replication attempts. In our computation, we take into account the likely scenario that publication bias had distorted the originally published results. Overall, 75{\%} of studies gave qualitatively similar results in terms of the amount of evidence provided. However, the evidence was often weak (i.e., Bayes factor {\textless} 10). The majority of the studies (64{\%}) did not provide strong evidence for either the null or the alternative hypothesis in either the original or the replication, and no replication attempts provided strong evidence in favor of the null. In all cases where the original paper provided strong evidence but the replication did not (15{\%}), the sample size in the replication was smaller than the original. Where the replication provided strong evidence but the original did not (10{\%}), the replication sample size was larger. We conclude that the apparent failure of the Reproducibility Project to replicate many target effects can be adequately explained by overestimation of effect sizes (or overestimation of evidence against the null hypothesis) due to small sample sizes and publication bias in the psychological literature. We further conclude that traditional sample sizes are insufficient and that a more widespread adoption of Bayesian methods is desirable.},
author = {Etz, Alexander and Vandekerckhove, Joachim},
doi = {10.1371/journal.pone.0149794},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {2},
pmid = {26919473},
title = {{A Bayesian perspective on the reproducibility project: Psychology}},
volume = {11},
year = {2016}
}

@article{Chambers2014,
abstract = {The last ten years have witnessed increasing awareness of questionable research practices (QRPs) in the life sciences [1,2], including p-hacking [3], HARKing [4], lack of replication [5], publication bias [6], low statistical power [7] and lack of data sharing ([8]; see Figure 1). Concerns about such behaviours have been raised repeatedly for over half a century [9–11] but the incentive structure of academia has not changed to address them.},
author = {Chambers, Christopher D. and Feredoes, Eva and {Muthukumaraswamy, Suresh}, D. and Etchells, Peter J.},
doi = {10.3934/Neuroscience.2014.1.4},
issn = {2373-7972},
journal = {AIMS Neuroscience},
number = {1},
pages = {4--17},
title = {{Instead of “playing the game” it is time to change the rules: Registered Reports at AIMS Neuroscience and beyond}},
volume = {1},
year = {2014}
}

@article{Szucs2016,
abstract = {We have empirically assessed the distribution of published effect sizes and estimated power by extracting more than 100,000 statistical records from about 10,000 cognitive neuroscience and psychology papers published during the past 5 years. The reported median effect size was d=0.93 (inter-quartile range: 0.64-1.46) for nominally statistically significant results and d=0.24 (0.11-0.42) for non-significant results. Median power to detect small, medium and large effects was 0.12, 0.44 and 0.73, reflecting no improvement through the past half-century. Power was lowest for cognitive neuroscience journals. 14{\%} of papers reported some statistically significant results, although the respective F statistic and degrees of freedom proved that these were non-significant; p value errors positively correlated with journal impact factors. False report probability is likely to exceed 50{\%} for the whole literature. In light of our findings the recently reported low replication success in psychology is realistic and worse performance may be expected for cognitive neuroscience.},
author = {Szucs, Denes and Ioannidis, John PA},
doi = {10.1101/071530},
isbn = {9788578110796},
issn = {1098-6596},
journal = {bioRxiv},
pages = {071530},
pmid = {25246403},
title = {{Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature}},
year = {2016}
}
